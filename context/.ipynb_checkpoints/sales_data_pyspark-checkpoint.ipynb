{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55549e6a-105a-4ed7-8f09-cdefc461b5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc22bfa0-70e4-4898-82b7-38aca1d18f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pyspark Version : 3.4.0\n",
      "Pyspark Version : 3.4.0\n"
     ]
    }
   ],
   "source": [
    "# Membuat SparkSession\n",
    "spark = SparkSession.builder.master(\"local[1]\")\\\n",
    "                    .appName('Pyspark job')\\\n",
    "                    .getOrCreate()\n",
    "\n",
    "# Menampilkan versi PySpark\n",
    "print('Pyspark Version : ' + spark.version)\n",
    "print('Pyspark Version : ' + spark.sparkContext.version)\n",
    "\n",
    "# Load Data SalesData\n",
    "df = spark.read.csv('sales_data.csv',header=True,sep=\";\",inferSchema=True)\n",
    "# temp table SalesData\n",
    "df.createOrReplaceTempView(\"temp_salesdata\")\n",
    "\n",
    "# Load Data Zipssortedbycitystate\n",
    "df2 = spark.read.csv('Zipssortedbycitystate.csv',header=True,sep=\";\",inferSchema=True)\n",
    "# temp table Zipssortedbycitystate\n",
    "df2.createOrReplaceTempView(\"temp_zipdata\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f19f63ec-71cf-4bdd-97e5-f6ea52a6ce8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+---------+---------------+-------+----------------+-------+------+--------+-------+----------------+----+-----------+--------------------+----------+--------------------+------------+-------------+-----+-------------+---------+---------------+----------------+\n",
      "|ORDERNUMBER|QUANTITYORDERED|PRICEEACH|ORDERLINENUMBER|  SALES|       ORDERDATE| STATUS|QTR_ID|MONTH_ID|YEAR_ID|     PRODUCTLINE|MSRP|PRODUCTCODE|        CUSTOMERNAME|     PHONE|        ADDRESSLINE1|ADDRESSLINE2|         CITY|STATE|      COUNTRY|TERRITORY|CONTACTLASTNAME|CONTACTFIRSTNAME|\n",
      "+-----------+---------------+---------+---------------+-------+----------------+-------+------+--------+-------+----------------+----+-----------+--------------------+----------+--------------------+------------+-------------+-----+-------------+---------+---------------+----------------+\n",
      "|      10159|             49|    100.0|             14|5205.27|10/10/2003 00:00|Shipped|     4|      10|   2003|     Motorcycles|  95|   S10_1678|Corporate Gift Id...|6505551386|     7734 Strong St.|        null|San Francisco|   CA|United States|       NA|          Brown|           Julie|\n",
      "|      10201|             22|    98.57|              2|2168.54|12/01/2003 00:00|Shipped|     4|      12|   2003|     Motorcycles|  95|   S10_1678|     Mini Wheels Co.|6505555787|5557 North Pendal...|        null|San Francisco|   CA|United States|       NA|         Murphy|           Julie|\n",
      "|      10333|             26|    100.0|              3| 3003.0| 11/18/2004 0:00|Shipped|     4|      11|   2004|    Classic Cars| 214|   S10_1949|     Mini Wheels Co.|6505555787|5557 North Pendal...|        null|San Francisco|   CA|United States|       NA|         Murphy|           Julie|\n",
      "|      10381|             36|    100.0|              3| 8254.8|  2/17/2005 0:00|Shipped|     1|       2|   2005|    Classic Cars| 214|   S10_1949|Corporate Gift Id...|6505551386|     7734 Strong St.|        null|San Francisco|   CA|          USA|       NA|          Brown|           Julie|\n",
      "|      10159|             37|    100.0|             17|5016.83|10/10/2003 00:00|Shipped|     4|      10|   2003|     Motorcycles| 118|   S10_2016|Corporate Gift Id...|6505551386|     7734 Strong St.|        null|San Francisco|   CA|          USA|       NA|          Brown|           Julie|\n",
      "|      10201|             24|    100.0|              5|3025.92|12/01/2003 00:00|Shipped|     4|      12|   2003|     Motorcycles| 118|   S10_2016|     Mini Wheels Co.|6505555787|5557 North Pendal...|        null|San Francisco|   CA|          USA|       NA|         Murphy|           Julie|\n",
      "|      10159|             22|    100.0|             16| 4132.7|10/10/2003 00:00|Shipped|     4|      10|   2003|     Motorcycles| 193|   S10_4698|Corporate Gift Id...|6505551386|     7734 Strong St.|        null|San Francisco|   CA|          USA|       NA|          Brown|           Julie|\n",
      "|      10201|             49|    100.0|              4|8065.89|12/01/2003 00:00|Shipped|     4|      12|   2003|     Motorcycles| 193|   S10_4698|     Mini Wheels Co.|6505555787|5557 North Pendal...|        null|San Francisco|   CA|          USA|       NA|         Murphy|           Julie|\n",
      "|      10209|             39|    100.0|              8|5197.92|01/09/2004 00:00|Shipped|     1|       1|   2004|    Classic Cars| 136|   S10_4757|Men 'R' US Retail...|2155554369|    6047 Douglas Av.|        null|  Los Angeles|   CA|          USA|       NA|       Chandler|         Michael|\n",
      "|      10384|             34|    100.0|              4| 4846.7|  2/23/2005 0:00|Shipped|     1|       2|   2005|    Classic Cars| 136|   S10_4757|Corporate Gift Id...|6505551386|     7734 Strong St.|        null|San Francisco|   CA|          USA|       NA|          Brown|           Julie|\n",
      "|      10381|             37|    100.0|              6|6231.54|  2/17/2005 0:00|Shipped|     1|       2|   2005|    Classic Cars| 147|   S10_4962|Corporate Gift Id...|6505551386|     7734 Strong St.|        null|San Francisco|   CA|          USA|       NA|          Brown|           Julie|\n",
      "|      10159|             41|    100.0|              2|8296.35|10/10/2003 00:00|Shipped|     4|      10|   2003|    Classic Cars| 194|   S12_1099|Corporate Gift Id...|6505551386|     7734 Strong St.|        null|San Francisco|   CA|          USA|       NA|          Brown|           Julie|\n",
      "|      10333|             33|    99.21|              6|3273.93| 11/18/2004 0:00|Shipped|     4|      11|   2004|Trucks and Buses| 136|   S12_1666|     Mini Wheels Co.|6505555787|5557 North Pendal...|        null|San Francisco|   CA|          USA|       NA|         Murphy|           Julie|\n",
      "|      10381|             20|    100.0|              1| 2952.0|  2/17/2005 0:00|Shipped|     1|       2|   2005|Trucks and Buses| 136|   S12_1666|Corporate Gift Id...|6505551386|     7734 Strong St.|        null|San Francisco|   CA|          USA|       NA|          Brown|           Julie|\n",
      "|      10159|             38|    100.0|             13|6238.84|10/10/2003 00:00|Shipped|     4|      10|   2003|     Motorcycles| 150|   S12_2823|Corporate Gift Id...|6505551386|     7734 Strong St.|        null|San Francisco|   CA|          USA|       NA|          Brown|           Julie|\n",
      "|      10201|             25|    100.0|              1| 4029.0|12/01/2003 00:00|Shipped|     4|      12|   2003|     Motorcycles| 150|   S12_2823|     Mini Wheels Co.|6505555787|5557 North Pendal...|        null|San Francisco|   CA|          USA|       NA|         Murphy|           Julie|\n",
      "|      10160|             46|    100.0|              6|5294.14|10/11/2003 00:00|Shipped|     4|      10|   2003|    Classic Cars| 117|   S12_3380|Men 'R' US Retail...|2155554369|    6047 Douglas Av.|        null|  Los Angeles|   CA|          USA|       NA|       Chandler|         Michael|\n",
      "|      10159|             24|    73.42|              3|1762.08|10/10/2003 00:00|Shipped|     4|      10|   2003|    Classic Cars|  79|   S12_3990|Corporate Gift Id...|6505551386|     7734 Strong St.|        null|San Francisco|   CA|          USA|       NA|          Brown|           Julie|\n",
      "|      10160|             50|    100.0|              5| 5182.0|10/11/2003 00:00|Shipped|     4|      10|   2003|    Classic Cars| 115|   S12_4675|Men 'R' US Retail...|2155554369|    6047 Douglas Av.|        null|  Los Angeles|   CA|          USA|       NA|       Chandler|         Michael|\n",
      "|      10333|             29|    40.25|              7|1167.25| 11/18/2004 0:00|Shipped|     4|      11|   2004|Trucks and Buses| 116|   S18_1097|     Mini Wheels Co.|6505555787|5557 North Pendal...|        null|San Francisco|   CA|          USA|       NA|         Murphy|           Julie|\n",
      "+-----------+---------------+---------+---------------+-------+----------------+-------+------+--------+-------+----------------+----+-----------+--------------------+----------+--------------------+------------+-------------+-----+-------------+---------+---------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----------+---------------+---------+---------------+-------+----------------+-------+------+--------+-------+-----------+----+-----------+--------------------+----------------+--------------------+------------+-----------+--------+----------+-------------+---------+---------------+----------------+\n",
      "|ORDERNUMBER|QUANTITYORDERED|PRICEEACH|ORDERLINENUMBER|  SALES|       ORDERDATE| STATUS|QTR_ID|MONTH_ID|YEAR_ID|PRODUCTLINE|MSRP|PRODUCTCODE|        CUSTOMERNAME|           PHONE|        ADDRESSLINE1|ADDRESSLINE2|       CITY|   STATE|POSTALCODE|      COUNTRY|TERRITORY|CONTACTLASTNAME|CONTACTFIRSTNAME|\n",
      "+-----------+---------------+---------+---------------+-------+----------------+-------+------+--------+-------+-----------+----+-----------+--------------------+----------------+--------------------+------------+-----------+--------+----------+-------------+---------+---------------+----------------+\n",
      "|      10107|             30|     95.7|              2| 2871.0|  2/24/2003 0:00|Shipped|     1|       2|   2003|Motorcycles|  95|   S10_1678|   Land of Toys Inc.|      2125557818|897 Long Airport ...|        null|        NYC|      NY|     10022|United States|       NA|             Yu|            Kwai|\n",
      "|      10121|             34|    81.35|              5| 2765.9|05/07/2003 00:00|Shipped|     2|       5|   2003|Motorcycles|  95|   S10_1678|  Reims Collectables|      26.47.1555|  59 rue de l'Abbaye|        null|      Reims|    null|     51100|       France|     EMEA|        Henriot|            Paul|\n",
      "|      10134|             41|    94.74|              2|3884.34|07/01/2003 00:00|Shipped|     3|       7|   2003|Motorcycles|  95|   S10_1678|     Lyon Souveniers|+33 1 46 62 7555|27 rue du Colonel...|        null|      Paris|    null|     75508|       France|     EMEA|       Da Cunha|          Daniel|\n",
      "|      10145|             45|    83.26|              6| 3746.7|  8/25/2003 0:00|Shipped|     3|       8|   2003|Motorcycles|  95|   S10_1678|   Toys4GrownUps.com|      6265557265|  78934 Hillside Dr.|        null|   Pasadena|      CA|     90003|          USA|       NA|          Young|           Julie|\n",
      "|      10168|             36|    96.66|              1|3479.76| 10/28/2003 0:00|Shipped|     4|      10|   2003|Motorcycles|  95|   S10_1678|Technics Stores Inc.|      6505556809|   9408 Furth Circle|        null| Burlingame|      CA|     94217|          USA|       NA|         Hirano|            Juri|\n",
      "|      10180|             29|    86.13|              9|2497.77|11/11/2003 00:00|Shipped|     4|      11|   2003|Motorcycles|  95|   S10_1678|Daedalus Designs ...|      20.16.1555|184, chauss�e de ...|        null|      Lille|    null|     59000|       France|     EMEA|          Ranc�|         Martine|\n",
      "|      10188|             48|    100.0|              1|5512.32| 11/18/2003 0:00|Shipped|     4|      11|   2003|Motorcycles|  95|   S10_1678|        Herkku Gifts|   +47 2267 3215|Drammen 121, PR 7...|        null|     Bergen|    null|    N 5804|       Norway|     EMEA|         Oeztan|          Veysel|\n",
      "|      10211|             41|    100.0|             14|4708.44|  1/15/2004 0:00|Shipped|     1|       1|   2004|Motorcycles|  95|   S10_1678|   Auto Canal+ Petit|  (1) 47.55.6555|   25, rue Lauriston|        null|      Paris|    null|     75016|       France|     EMEA|        Perrier|       Dominique|\n",
      "|      10223|             37|    100.0|              1|3965.66|  2/20/2004 0:00|Shipped|     1|       2|   2004|Motorcycles|  95|   S10_1678|Australian Collec...|    03 9520 4555|   636 St Kilda Road|     Level 3|  Melbourne|Victoria|      3004|    Australia|     APAC|       Ferguson|           Peter|\n",
      "|      10237|             23|    100.0|              7|2333.12|04/05/2004 00:00|Shipped|     2|       4|   2004|Motorcycles|  95|   S10_1678|     Vitachrome Inc.|      2125551500|   2678 Kingston Rd.|   Suite 101|        NYC|      NY|     10022|          USA|       NA|          Frick|         Michael|\n",
      "|      10251|             28|    100.0|              2|3188.64|  5/18/2004 0:00|Shipped|     2|       5|   2004|Motorcycles|  95|   S10_1678|Tekni Collectable...|      2015559350|       7476 Moss Rd.|        null|     Newark|      NJ|     94019|          USA|       NA|          Brown|         William|\n",
      "|      10263|             34|    100.0|              2|3676.76|  6/28/2004 0:00|Shipped|     2|       6|   2004|Motorcycles|  95|   S10_1678|     Gift Depot Inc.|      2035552570| 25593 South Bay Ln.|        null|Bridgewater|      CT|     97562|United States|       NA|           King|           Julie|\n",
      "|      10275|             45|    92.83|              1|4177.35|  7/23/2004 0:00|Shipped|     3|       7|   2004|Motorcycles|  95|   S10_1678|   La Rochelle Gifts|      40.67.8555|67, rue des Cinqu...|        null|     Nantes|    null|     44000|       France|     EMEA|        Labrune|          Janine|\n",
      "|      10285|             36|    100.0|              6|4099.68|  8/27/2004 0:00|Shipped|     3|       8|   2004|Motorcycles|  95|   S10_1678|Marta's Replicas Co.|      6175558555| 39323 Spinnaker Dr.|        null|  Cambridge|      MA|     51247|          USA|       NA|      Hernandez|           Marta|\n",
      "|      10299|             23|    100.0|              9|2597.39|  9/30/2004 0:00|Shipped|     3|       9|   2004|Motorcycles|  95|   S10_1678|Toys of Finland, Co.|     90-224 8555|       Keskuskatu 45|        null|   Helsinki|    null|     21240|      Finland|     EMEA|      Karttunen|           Matti|\n",
      "|      10309|             41|    100.0|              5|4394.38| 10/15/2004 0:00|Shipped|     4|      10|   2004|Motorcycles|  95|   S10_1678|  Baane Mini Imports|      07-98 9555|Erling Skakkes ga...|        null|    Stavern|    null|      4110|       Norway|     EMEA|     Bergulfsen|           Jonas|\n",
      "|      10318|             46|    94.74|              1|4358.04|11/02/2004 00:00|Shipped|     4|      11|   2004|Motorcycles|  95|   S10_1678|Diecast Classics ...|      2155551555|    7586 Pompton St.|        null|  Allentown|      PA|     70267|          USA|       NA|             Yu|           Kyung|\n",
      "|      10329|             42|    100.0|              1|4396.14| 11/15/2004 0:00|Shipped|     4|      11|   2004|Motorcycles|  95|   S10_1678|   Land of Toys Inc.|      2125557818|897 Long Airport ...|        null|        NYC|      NY|     10022|United States|       NA|             Yu|            Kwai|\n",
      "|      10341|             41|    100.0|              9|7737.93| 11/24/2004 0:00|Shipped|     4|      11|   2004|Motorcycles|  95|   S10_1678|Salzburg Collecta...|       6562-9555|         Geislweg 14|        null|   Salzburg|    null|      5020|      Austria|     EMEA|          Pipps|           Georg|\n",
      "|      10361|             20|    72.55|             13| 1451.0| 12/17/2004 0:00|Shipped|     4|      12|   2004|Motorcycles|  95|   S10_1678|Souveniers And Th...| +61 2 9495 8555|Monitor Money Bui...|     Level 6|  Chatswood|     NSW|      2067|    Australia|     APAC|         Huxley|          Adrian|\n",
      "+-----------+---------------+---------+---------------+-------+----------------+-------+------+--------+-------+-----------+----+-----------+--------------------+----------------+--------------------+------------+-----------+--------+----------+-------------+---------+---------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cari postalcode yang null dari sales data\n",
    "df = spark.sql(\"select * from temp_salesdata where postalcode is null\")\n",
    "\n",
    "# hapus postal code karena bernilai null atau kosong\n",
    "df = df.drop(\"postalcode\")\n",
    "\n",
    "# buat temp baru untuk menyimpan hasil yang postalcode = null\n",
    "df.createOrReplaceTempView(\"temp_postalcodenull\")\n",
    "\n",
    "# cari data yang postalcode nya tidak null\n",
    "df2 = spark.sql(\"select * from temp_salesdata where postalcode is not null\")\n",
    "\n",
    "# simpan data yang postalcode nya tidak null ke temp baru\n",
    "df2.createOrReplaceTempView(\"temp_postalnotcodenull\")\n",
    "df.show()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f826a0a-4870-4022-8828-0d27b8889c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+---------+---------------+-------+----------------+-------+------+--------+-------+----------------+----+-----------+--------------------+----------+--------------------+------------+-------------+-----+-------+---------+---------------+----------------+----------+\n",
      "|ORDERNUMBER|QUANTITYORDERED|PRICEEACH|ORDERLINENUMBER|  SALES|       ORDERDATE| STATUS|QTR_ID|MONTH_ID|YEAR_ID|     PRODUCTLINE|MSRP|PRODUCTCODE|        CUSTOMERNAME|     PHONE|        ADDRESSLINE1|ADDRESSLINE2|         CITY|STATE|COUNTRY|TERRITORY|CONTACTLASTNAME|CONTACTFIRSTNAME|POSTALCODE|\n",
      "+-----------+---------------+---------+---------------+-------+----------------+-------+------+--------+-------+----------------+----+-----------+--------------------+----------+--------------------+------------+-------------+-----+-------+---------+---------------+----------------+----------+\n",
      "|      10209|             48|    44.69|              3|2145.12|01/09/2004 00:00|Shipped|     1|       1|   2004|          Planes|  49|   S72_1253|Men 'R' US Retail...|2155554369|    6047 Douglas Av.|        null|  Los Angeles|   CA|    USA|       NA|       Chandler|         Michael|     90XXX|\n",
      "|      10209|             33|    88.71|              4|2927.43|01/09/2004 00:00|Shipped|     1|       1|   2004|           Ships|  90|  S700_2047|Men 'R' US Retail...|2155554369|    6047 Douglas Av.|        null|  Los Angeles|   CA|    USA|       NA|       Chandler|         Michael|     90XXX|\n",
      "|      10160|             35|    100.0|              3| 4767.7|10/11/2003 00:00|Shipped|     4|      10|   2003|    Classic Cars| 140|   S24_3856|Men 'R' US Retail...|2155554369|    6047 Douglas Av.|        null|  Los Angeles|   CA|    USA|       NA|       Chandler|         Michael|     90XXX|\n",
      "|      10209|             22|    89.73|              7|1974.06|01/09/2004 00:00|Shipped|     1|       1|   2004|    Vintage Cars|  83|   S24_3816|Men 'R' US Retail...|2155554369|    6047 Douglas Av.|        null|  Los Angeles|   CA|    USA|       NA|       Chandler|         Michael|     90XXX|\n",
      "|      10209|             36|    77.59|              2|2793.24|01/09/2004 00:00|Shipped|     1|       1|   2004|    Vintage Cars|  65|   S24_3420|Men 'R' US Retail...|2155554369|    6047 Douglas Av.|        null|  Los Angeles|   CA|    USA|       NA|       Chandler|         Michael|     90XXX|\n",
      "|      10160|             42|     37.0|              2| 1554.0|10/11/2003 00:00|Shipped|     4|      10|   2003|    Classic Cars|  37|   S24_2972|Men 'R' US Retail...|2155554369|    6047 Douglas Av.|        null|  Los Angeles|   CA|    USA|       NA|       Chandler|         Michael|     90XXX|\n",
      "|      10209|             43|    82.21|              1|3535.03|01/09/2004 00:00|Shipped|     1|       1|   2004|          Planes|  68|   S24_2841|Men 'R' US Retail...|2155554369|    6047 Douglas Av.|        null|  Los Angeles|   CA|    USA|       NA|       Chandler|         Michael|     90XXX|\n",
      "|      10209|             20|    100.0|              5| 2498.6|01/09/2004 00:00|Shipped|     1|       1|   2004|    Vintage Cars| 105|   S18_3856|Men 'R' US Retail...|2155554369|    6047 Douglas Av.|        null|  Los Angeles|   CA|    USA|       NA|       Chandler|         Michael|     90XXX|\n",
      "|      10160|             20|    100.0|              1| 3996.4|10/11/2003 00:00|Shipped|     4|      10|   2003|    Classic Cars| 169|   S18_3232|Men 'R' US Retail...|2155554369|    6047 Douglas Av.|        null|  Los Angeles|   CA|    USA|       NA|       Chandler|         Michael|     90XXX|\n",
      "|      10209|             28|    100.0|              6|2817.92|01/09/2004 00:00|Shipped|     1|       1|   2004|           Ships|  86|   S18_3029|Men 'R' US Retail...|2155554369|    6047 Douglas Av.|        null|  Los Angeles|   CA|    USA|       NA|       Chandler|         Michael|     90XXX|\n",
      "|      10160|             38|    88.55|              4| 3364.9|10/11/2003 00:00|Shipped|     4|      10|   2003|    Classic Cars|  77|   S18_1889|Men 'R' US Retail...|2155554369|    6047 Douglas Av.|        null|  Los Angeles|   CA|    USA|       NA|       Chandler|         Michael|     90XXX|\n",
      "|      10160|             50|    100.0|              5| 5182.0|10/11/2003 00:00|Shipped|     4|      10|   2003|    Classic Cars| 115|   S12_4675|Men 'R' US Retail...|2155554369|    6047 Douglas Av.|        null|  Los Angeles|   CA|    USA|       NA|       Chandler|         Michael|     90XXX|\n",
      "|      10160|             46|    100.0|              6|5294.14|10/11/2003 00:00|Shipped|     4|      10|   2003|    Classic Cars| 117|   S12_3380|Men 'R' US Retail...|2155554369|    6047 Douglas Av.|        null|  Los Angeles|   CA|    USA|       NA|       Chandler|         Michael|     90XXX|\n",
      "|      10209|             39|    100.0|              8|5197.92|01/09/2004 00:00|Shipped|     1|       1|   2004|    Classic Cars| 136|   S10_4757|Men 'R' US Retail...|2155554369|    6047 Douglas Av.|        null|  Los Angeles|   CA|    USA|       NA|       Chandler|         Michael|     90XXX|\n",
      "|      10384|             49|    100.0|              1|6397.44|  2/23/2005 0:00|Shipped|     1|       2|   2005|           Ships|  86|  S700_1938|Corporate Gift Id...|6505551386|     7734 Strong St.|        null|San Francisco|   CA|    USA|       NA|          Brown|           Julie|     941XX|\n",
      "|      10159|             31|     71.6|             10| 2219.6|10/10/2003 00:00|Shipped|     4|      10|   2003|     Motorcycles|  81|   S50_4713|Corporate Gift Id...|6505551386|     7734 Strong St.|        null|San Francisco|   CA|    USA|       NA|          Brown|           Julie|     941XX|\n",
      "|      10159|             23|    100.0|             12|2347.15|10/10/2003 00:00|Shipped|     4|      10|   2003|     Motorcycles| 102|   S32_4485|Corporate Gift Id...|6505551386|     7734 Strong St.|        null|San Francisco|   CA|    USA|       NA|          Brown|           Julie|     941XX|\n",
      "|      10333|             33|    73.69|              4|2431.77| 11/18/2004 0:00|Shipped|     4|      11|   2004|Trucks and Buses|  64|   S32_3522|     Mini Wheels Co.|6505555787|5557 North Pendal...|        null|San Francisco|   CA|    USA|       NA|         Murphy|           Julie|     941XX|\n",
      "|      10159|             35|     35.4|              9| 1239.0|10/10/2003 00:00|Shipped|     4|      10|   2003|     Motorcycles|  40|   S32_2206|Corporate Gift Id...|6505551386|     7734 Strong St.|        null|San Francisco|   CA|    USA|       NA|          Brown|           Julie|     941XX|\n",
      "|      10159|             23|     67.1|              6| 1543.3|10/10/2003 00:00|Shipped|     4|      10|   2003|    Classic Cars|  80|   S24_4620|Corporate Gift Id...|6505551386|     7734 Strong St.|        null|San Francisco|   CA|    USA|       NA|          Brown|           Julie|     941XX|\n",
      "+-----------+---------------+---------+---------------+-------+----------------+-------+------+--------+-------+----------------+----+-----------+--------------------+----------+--------------------+------------+-------------+-----+-------+---------+---------------+----------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sql dalam CTE_ZIPCODE merupakan proses mengubah digit terakhir postalcode sesuai selisih digit yang memiliki city dan state yang sama\n",
    "# selanjutnya ialah memasukkan kolom POSTALCODE ke dalam temp_postalcodenull(table dengan postalcode == null) sesuai city dan state yang sama\n",
    "df = spark.sql('''\n",
    "            WITH CTE_ZIPCODE AS (\n",
    "                    SELECT \n",
    "                        city, \n",
    "                        state, \n",
    "                        MAX(postalcode) - MIN(postalcode) AS jum,\n",
    "                        LENGTH(CAST(MAX(postalcode) - MIN(postalcode) AS STRING)) AS jumlah_digit_diganti,\n",
    "                        FIRST(postalcode) AS first_postalcode,\n",
    "                        CONCAT(\n",
    "                            SUBSTRING(FIRST(postalcode), 1, 5 - LENGTH(CAST(MAX(postalcode) - MIN(postalcode) AS STRING))),\n",
    "                            REPEAT('X', LENGTH(CAST(MAX(postalcode) - MIN(postalcode) AS STRING)))\n",
    "                        ) AS POSTALCODE\n",
    "                    FROM temp_zipdata \n",
    "                    GROUP BY city, state\n",
    "                )\n",
    "                SELECT\n",
    "                    tn.*,\n",
    "                    cte.POSTALCODE\n",
    "                FROM temp_postalcodenull tn\n",
    "                JOIN CTE_ZIPCODE cte\n",
    "                ON tn.city = cte.city AND tn.state = cte.state\n",
    "                ''')\n",
    "df.show()\n",
    "# simpan hasil pengabungan ke dalam temp baru yaitu temp_hasil\n",
    "df.createOrReplaceTempView(\"temp_hasil\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb81f606-8fb5-40b0-bb44-83a4550b7bdd",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o106.csv.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:354)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:382)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:354)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:847)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:341)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:331)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:370)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:955)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:192)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:215)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1111)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1120)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 23 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12280\\3351535612.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# Menyimpan dataframe hasil union ke file .csv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mdf_union\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'OutputSpark.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'overwrite'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mdf_union\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[1;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[0;32m   1797\u001b[0m             \u001b[0mlineSep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlineSep\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1798\u001b[0m         )\n\u001b[1;32m-> 1799\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1800\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1801\u001b[0m     def orc(\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o106.csv.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:354)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:382)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:354)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:847)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:341)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:331)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:370)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:955)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:192)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:215)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1111)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1120)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 23 more\r\n"
     ]
    }
   ],
   "source": [
    "# Mendapatkan daftar kolom dari temp_postalnotcodenull\n",
    "columns_temp_postalnotcodenull = spark.table(\"temp_postalnotcodenull\").columns\n",
    "\n",
    "# Menentukan kolom secara eksplisit sesuai urutan dari temp_postalnotcodenull\n",
    "df_union = spark.sql(f'''\n",
    "    SELECT {', '.join(columns_temp_postalnotcodenull)}\n",
    "    FROM temp_postalnotcodenull\n",
    "    UNION\n",
    "    SELECT {', '.join(columns_temp_postalnotcodenull)}\n",
    "    FROM temp_hasil\n",
    "''')\n",
    "# outputHasil merupakan tabel akhir\n",
    "df_union.createOrReplaceTempView(\"outputHasil\")\n",
    "\n",
    "# csv\n",
    "df_union.write.csv('sales_csv/OutputSpark.csv', header=True, mode='overwrite')\n",
    "\n",
    "df_union.show()\n",
    "\n",
    "df2 = spark.sql(\"SELECT * FROM outputHasil where city = 'San Francisco'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec38ed6d-20cf-4a37-8aa8-05751b338096",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
