from pyspark.sql import SparkSession


!where python


# import os


# # # Path ke interpreter Python
# python_path = "C:\\anaconda3\\python.exe"

# # # Atur variabel lingkungan
# os.environ['PYSPARK_PYTHON'] = python_path
# os.environ['PYSPARK_DRIVER_PYTHON'] = python_path
# os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = 'notebook'

# from pyspark.sql import SparkSession
# Membuat SparkSession
spark = SparkSession.builder.master("local[1]")\
                    .appName('Pyspark job')\
                    .getOrCreate()



# spark



# Menampilkan versi PySpark
print('Pyspark Version : ' + spark.version)
print('Pyspark Version : ' + spark.sparkContext.version)

# Membuat RDD dari list
RDD1 = spark.sparkContext.parallelize(["One", "Two", "Three"])
print(type(RDD1))


print(RDD1.collect())


RDD2 = RDD1.map(lambda x: x.lower())
print(RDD2.collect())

RDD1 = spark.sparkContext.textFile("context/data.txt")
RDD2 = RDD1.map(lambda x:x.lower())
print(RDD2.collect())

def my_custom_func(x):
    return x.lower()
    RDD = RDD1.map(my_custom_func)
    print(RDD2.collect())

RDD1 = spark.sparkContext.textFile("context/data.txt")
RDD2 = RDD1.map(lambda x: x.split(" "))
print("RDD2: ", RDD2.collect())

RDD3 = RDD1.flatMap(lambda x: x.split(" "))
print("RDD3: ", RDD3.collect())

RDD4 = RDD1.map(lambda x: x)
print("RDD4: ", RDD4.collect())

RDD5 = RDD1.flatMap(lambda x: x)
print("RDD5: ", RDD5.collect())




