{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k7D4ZBWoWvVo",
    "outputId": "cdea8e74-5f03-4fbc-a273-2a136e8122ff"
   },
   "outputs": [],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MCVONJpyWQ06"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AGpXgSVRWjpw",
    "outputId": "e07cf1dc-4c75-43fd-8a94-2ac0a1358f1e"
   },
   "outputs": [],
   "source": [
    "# Membuat SparkSession\n",
    "spark = SparkSession.builder.master(\"local[1]\")\\\n",
    "                    .appName('Pyspark job')\\\n",
    "                    .getOrCreate()\n",
    "\n",
    "# Menampilkan versi PySpark\n",
    "print('Pyspark Version : ' + spark.version)\n",
    "print('Pyspark Version : ' + spark.sparkContext.version)\n",
    "\n",
    "# Membuat RDD dari list\n",
    "RDD1 = spark.sparkContext.parallelize([\"One\", \"Two\", \"Three\"])\n",
    "print(type(RDD1))\n",
    "\n",
    "RDD2 = RDD1.map(lambda x: x.lower())\n",
    "print(RDD2.collect())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gERYexTTXsGU",
    "outputId": "d01f683a-dcf3-44b0-c311-8344cfd549ef"
   },
   "outputs": [],
   "source": [
    "RDD1 = spark.sparkContext.textFile(\"../context/data.txt\")\n",
    "RDD2 = RDD1.map(lambda x:x.lower())\n",
    "print(RDD2.collect())\n",
    "\n",
    "def my_custom_func(x):\n",
    "    return x.lower()\n",
    "    RDD = RDD1.map(my_custom_func)\n",
    "    print(RDD2.collect())\n",
    "\n",
    "RDD1 = spark.sparkContext.textFile(\"../context/data.txt\")\n",
    "RDD2 = RDD1.map(lambda x:x.split(\" \"))\n",
    "print(\"RDD2: \",RDD2.collect())\n",
    "\n",
    "RDD3 = RDD1.flatMap(lambda x:x.split(\" \"))\n",
    "print(\"RDD3: \",RDD3.collect())\n",
    "\n",
    "RDD4 = RDD1.map(lambda x:x)\n",
    "print(\"RDD4: \",RDD4.collect())\n",
    "\n",
    "RDD5 = RDD1.flatMap(lambda x:x)\n",
    "print(\"RDD5: \",RDD5.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dgl0G-51h5X7",
    "outputId": "4c7023ea-189d-4d5b-cffd-cb7335bec34c"
   },
   "outputs": [],
   "source": [
    "stopwords = ['is','am','are','the','for','a']\n",
    "RDD1 = spark.sparkContext.parallelize([\"I\",\"am\",\"a\",\"student\"])\n",
    "RDD2 = RDD1.filter(lambda x:x not in stopwords)\n",
    "print(\"RDD2:\", RDD2.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M8RPk79cilbJ",
    "outputId": "f35247c0-040d-4663-97db-6960b399a805"
   },
   "outputs": [],
   "source": [
    "sentence = spark.sparkContext.parallelize([\"I like apple and apple\"])\n",
    "list_of_words = sentence.flatMap(lambda x:x.split(\" \"))\n",
    "print(\"list of words :\", list_of_words.collect())\n",
    "key_val = list_of_words.map(lambda x: (x.lower(),1))\n",
    "print(\"Key_val :\",key_val.collect())\n",
    "key_val_reduced = key_val.reduceByKey(lambda x,y: x+y)\n",
    "print(\"key_val_reduced: \",key_val_reduced.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kaebOSHujbkD",
    "outputId": "86766a47-9c83-4fb0-ce71-cad550536565"
   },
   "outputs": [],
   "source": [
    "sorted = key_val_reduced.sortByKey()\n",
    "print(\"sorted:\", sorted.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rxLOYckDjl53",
    "outputId": "627f205a-1697-4752-99a2-83733e4efe0d"
   },
   "outputs": [],
   "source": [
    "words_lenght = list_of_words.map(lambda s: len(s))\n",
    "print(\"Words lenght :\",words_lenght.collect())\n",
    "totallenght = words_lenght.reduce(lambda a,b : a+b)\n",
    "print(totallenght)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Bpoy9ERj7ZU",
    "outputId": "ade52e66-1b9a-4f73-bb18-537ba7b9b617"
   },
   "outputs": [],
   "source": [
    "print(\"collect\", list_of_words.collect())\n",
    "print(\"take-2\",sorted.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HDmW0SqzlIpC",
    "outputId": "3d54ee20-4156-48de-dceb-8c48fc4ae0f3"
   },
   "outputs": [],
   "source": [
    "RDD1 = spark.sparkContext.textFile(\"../context/pridenprejudice.txt\")\n",
    "RDD2 = spark.sparkContext.textFile(\"../context/stopwords.txt\")\n",
    "# RDD2.collect()\n",
    "\n",
    "\n",
    "list_of_words = RDD1.flatMap(lambda x:x.split(\" \"))\n",
    "stopwords = set(RDD2.flatMap(lambda x: x.split()).map(lambda x: x.lower()).collect())\n",
    "\n",
    "filtered_words = list_of_words.filter(lambda x: x.lower() not in stopwords)\n",
    "\n",
    "# Menghitung frekuensi kata\n",
    "word_counts = filtered_words.map(lambda x: (x.lower(), 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Sort\n",
    "sorted = word_counts.sortByKey(ascending=True)\n",
    "\n",
    "# Menampilkan hasil hitung frekuensi kata\n",
    "print(\"Word counts:\", sorted.collect())\n",
    "\n",
    "words_lenght = filtered_words.map(lambda s: len(s))\n",
    "totallenght = words_lenght.reduce(lambda a,b : a+b)\n",
    "print(\"Total Huruf:\",totallenght)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Membaca file teks\n",
    "RDD1 = spark.sparkContext.textFile(\"../context/pridenprejudice.txt\")\n",
    "RDD2 = spark.sparkContext.textFile(\"../context/stopwords.txt\")\n",
    "\n",
    "# Menghapus simbol dari teks dan membagi menjadi kata-kata\n",
    "def clean_text(text):\n",
    "    return re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "\n",
    "list_of_words = RDD1.flatMap(lambda x: clean_text(x).split())\n",
    "\n",
    "# Membaca stopwords\n",
    "stopwords = set(RDD2.flatMap(lambda x: x.split()).map(lambda x: x.lower()).collect())\n",
    "\n",
    "# Filter stopwords\n",
    "filtered_words = list_of_words.filter(lambda x: x.lower() not in stopwords)\n",
    "\n",
    "# Menghitung frekuensi kata\n",
    "word_counts = filtered_words.map(lambda x: (x.lower(), 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Sort by frequency in descending order and take the top 10\n",
    "top_10_words = word_counts.takeOrdered(10, key=lambda x: -x[1])\n",
    "\n",
    "# Menampilkan 10 kata dengan frekuensi tertinggi\n",
    "print(\"Top 10 words:\", top_10_words)\n",
    "\n",
    "# Menghitung total panjang huruf\n",
    "words_length = filtered_words.map(lambda s: len(s))\n",
    "total_length = words_length.reduce(lambda a, b: a + b)\n",
    "print(\"Total Huruf:\", total_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "REsZS6whs59H"
   },
   "outputs": [],
   "source": [
    "list_of_words = RDD2.flatMap(lambda x:x.split(\" \"))\n",
    "key_val = list_of_words.map(lambda x: (x.lower(),1))\n",
    "key_val_reduced = key_val.reduceByKey(lambda x,y: x+y)\n",
    "\n",
    "\n",
    "sorted = key_val_reduced.sortByKey(ascending=True)\n",
    "top10 = sorted.take(10)\n",
    "print(\"sorted:\", sorted.collect())\n",
    "print(\"sorted top10:\", top10)\n",
    "\n",
    "words_lenght = list_of_words.map(lambda s: len(s))\n",
    "totallenght = words_lenght.reduce(lambda a,b : a+b)\n",
    "print(\"Total Huruf:\",totallenght)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8PidUzMUUnEp"
   },
   "outputs": [],
   "source": [
    "df_csv = spark.read.csv(f\"../context/cars.csv\",\n",
    "  header=True,\n",
    "  inferSchema=True,\n",
    "  sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6t_tlbAKVR6O",
    "outputId": "47338744-680d-4bd3-f1a3-e4cb5741224e"
   },
   "outputs": [],
   "source": [
    "df_csv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8l0wOhrFX_OW",
    "outputId": "90ac9176-d8cf-4671-989d-e2c83dc776e6"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# get the start time\n",
    "st = time.time()\n",
    "\n",
    "jumlahcar = df_csv.select(\"Model\")\n",
    "groupedcar = jumlahcar.groupBy(\"Model\").count()\n",
    "groupedcar.show()\n",
    "et = time.time()\n",
    "\n",
    "elapsed_time = et - st\n",
    "print('execution time', elapsed_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupedcar.write.mode('overwrite').csv(\"../context/output_count_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "glR0ZUk4Wqgx",
    "outputId": "d3271ad8-97ef-4913-f635-916555227aaa"
   },
   "outputs": [],
   "source": [
    "# top 10 lightest cars from US\n",
    "import time\n",
    "\n",
    "# get the start time\n",
    "st = time.time()\n",
    "\n",
    "top10lightcars = df_csv.select(\"Car\",\"Weight\")\n",
    "ordercar = top10lightcars.orderBy(\"Weight\", ascending=True)\n",
    "top10lightcars = ordercar.limit(10)\n",
    "top10lightcars.show()\n",
    "et = time.time()\n",
    "\n",
    "elapsed_time = et - st\n",
    "print('execution time', elapsed_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OYoId_d6Ze6-",
    "outputId": "cb462ec1-2bd2-4d8f-b25b-ce336d604e9b"
   },
   "outputs": [],
   "source": [
    "# top 10 cars with the most MPG\n",
    "import time\n",
    "\n",
    "# get the start time\n",
    "st = time.time()\n",
    "\n",
    "top10mpg = df_csv.select(\"Car\",\"MPG\")\n",
    "ordercar = top10mpg.orderBy(\"MPG\", ascending=False)\n",
    "top10mpg = ordercar.limit(10)\n",
    "top10mpg.show()\n",
    "et = time.time()\n",
    "\n",
    "elapsed_time = et - st\n",
    "print('execution time', elapsed_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QGlUmhMpZ6Ug",
    "outputId": "6c5510c0-61bf-4b4e-f301-9c7abf24b4eb"
   },
   "outputs": [],
   "source": [
    "# how many cars from each origin that comes in the 73\n",
    "import time\n",
    "\n",
    "# get the start time\n",
    "st = time.time()\n",
    "origincars = df_csv.select(\"Origin\",\"Model\")\n",
    "oc = origincars.filter(origincars.Model == \"73\")\n",
    "groupedcar = oc.groupBy(\"Origin\",\"Model\").count()\n",
    "groupedcar.show()\n",
    "et = time.time()\n",
    "\n",
    "elapsed_time = et - st\n",
    "print('execution time', elapsed_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mT0SgJ8AckbX",
    "outputId": "c94bf427-2696-4a46-a452-fdc1718919da"
   },
   "outputs": [],
   "source": [
    "# Top 10 most powerful cars horespower/weight ratio\n",
    "import time\n",
    "\n",
    "# get the start time\n",
    "st = time.time()\n",
    "ratio = (df_csv.Horsepower / df_csv.Weight)\n",
    "top10powerful = df_csv.select(\"Car\",ratio)\n",
    "ordercar = top10powerful.orderBy(ratio, ascending=False)\n",
    "top10powerful = ordercar.limit(10)\n",
    "top10powerful.show()\n",
    "et = time.time()\n",
    "\n",
    "elapsed_time = et - st\n",
    "print('execution time', elapsed_time, 'seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4vqqSTL8f87A",
    "outputId": "e2c87197-8dbf-4b6a-edf8-1fcd8bbf1764"
   },
   "outputs": [],
   "source": [
    "# best acceleration on each cilinder class\n",
    "import time\n",
    "\n",
    "# get the start time\n",
    "st = time.time()\n",
    "accel = df_csv.select(\"Cylinders\",\"Acceleration\")\n",
    "groupedcar = accel.groupBy(\"Cylinders\").max(\"Acceleration\")\n",
    "groupedcar.show()\n",
    "et = time.time()\n",
    "\n",
    "elapsed_time = et - st\n",
    "print('execution time', elapsed_time, 'seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "49sAWyyTgs8N",
    "outputId": "152f7a8e-68f5-4196-edd4-0d99a19904fe"
   },
   "outputs": [],
   "source": [
    "# best mpg one each displacement ckass(100cc,200cc,300cc,400cc)\n",
    "from pyspark.sql.functions import when, col, max as spark_max\n",
    "import time\n",
    "\n",
    "# get the start time\n",
    "st = time.time()\n",
    "displacement = df_csv.select(\"Displacement\",\"MPG\")\n",
    "categorized_displacement = displacement.withColumn(\n",
    "    \"DisplacementClass\",\n",
    "    when((col(\"Displacement\") >= 100) & (col(\"Displacement\") < 200), \"100cc\")\n",
    "    .when((col(\"Displacement\") >= 200) & (col(\"Displacement\") < 300), \"200cc\")\n",
    "    .when((col(\"Displacement\") >= 300) & (col(\"Displacement\") < 400), \"300cc\")\n",
    "    .when((col(\"Displacement\") >= 400) & (col(\"Displacement\") < 500), \"400cc\")\n",
    "    .otherwise(\"Other\")\n",
    ")\n",
    "# Kelompokkan berdasarkan kelas displacement dan temukan nilai MPG maksimal di setiap kelas\n",
    "grouped_displacement = categorized_displacement.groupBy(\"DisplacementClass\").agg(spark_max(\"MPG\").alias(\"MaxMPG\"))\n",
    "\n",
    "# Tampilkan hasilnya\n",
    "grouped_displacement.show()\n",
    "et = time.time()\n",
    "\n",
    "elapsed_time = et - st\n",
    "print('execution time', elapsed_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import fungsi concat(pengabungan kolom) dan lit(literal) \n",
    "from pyspark.sql.functions import concat, lit\n",
    "df = df_csv.withColumn('car_model',concat(col(\"Car\"), lit(\" \"), col(\"model\")))\n",
    "df.show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename\n",
    "from pyspark.sql.functions import lit\n",
    "df = df_csv.withColumn('first_column', lit(1))\\\n",
    "           .withColumn('second_column', lit(2))\\\n",
    "           .withColumn('third_column', lit('third_column'))\n",
    "\n",
    "df = df.withColumnRenamed('first_column', 'new_column_one')\\\n",
    "           .withColumnRenamed('second_column', 'new_column_two')\\\n",
    "           .withColumnRenamed('third_column', 'new_column_three')\n",
    "df.show(truncate=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count\n",
    "df.groupBy('Origin').count().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple count\n",
    "df.groupBy('Origin','Model').count().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum\n",
    "df.groupBy('Origin','Model').sum('MPG').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering rows in Pyspark \n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "total_count = df.count()\n",
    "print(\"Total Recourd Count :\" + str(total_count))\n",
    "europe_filtered_count = df.filter((col('Origin')=='Europe') & (col('Origin')>=100)).count() #Two condition added here\n",
    "print(\"Europe Filtered Record Count: \" + str(europe_filtered_count))\n",
    "df.filter(col('Origin')=='Europe').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Unique Rows\n",
    "df.select('Origin','Model').distinct().orderBy('Origin','Model').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Union\n",
    "df_csv = spark.read.csv('cars.csv',heafer=True,sep=\";\",inferschema=True)\n",
    "europe_cars = df_csv.filter((col('Origin')=='Europe') & (col('Cylinders') ==5))\n",
    "japan_cars = df_csv.filter((col('Origin')=='Japan') & (col('Cylinders') ==3))\n",
    "print(\"Europe Cars: \"+str(europe_cars.count()))\n",
    "print(\"Japan Cars: \"+str(japan.count()))\n",
    "print(\"Union Cars: \"+str(europe_cars.union(japan_cars).count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.createDataFrame([[1,2,3]],[\"col0\",\"col1\",\"col2\"])\n",
    "df2 = spark.createDataFrame([[4,5,6]],[\"col2\",\"col1\",\"col0\"])\n",
    "df1.unionByName(df2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df = spark.read.csv('../context/cars.csv',header=True,sep=\";\",inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,lower,upper,substring\n",
    "\n",
    "# help(substring)\n",
    "df.select(col('Car'),lower(col('Car'),upper(col('Car'),substring(col('Car'),1,4).alias(\"concatenated value\")))).show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min,max\n",
    "df.select(min(col('Weight'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_add, date_sub , min, max\n",
    "# create a dummy dataframe\n",
    "df = spark.createDataFrame([('1990-01-01',),('1995-01-03',),('2021-03-30',)], ['Date'])\n",
    "# find out the require data dates\n",
    "df.select(date_add(max(col('Date')),3),date_sub(min(col('Date')),3)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_df = spark.createDataFrame([[1,'Car A'],[2,'Car B'],[3,'Car C']],[\"id\",\"nama\"])\n",
    "cars_price_df = spark.createDataFrame([[1,1000],[2,2000],[3,3000],[5,5090]],[\"id\",\"car_price\"])\n",
    "cars_df.show()\n",
    "cars_price_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join\n",
    "cars_df.join(cars_price_df,cars_df.id == cars_price_df.id,'inner').select(cars_df['id'],cars_df['nama'],cars_price_df['car_price']).show()\n",
    "# cross\n",
    "cars_df.join(cars_price_df,cars_df.id == cars_price_df.id,'cross').select(cars_df['id'],cars_df['nama'],cars_price_df['car_price']).show()\n",
    "# left\n",
    "cars_df.join(cars_price_df,cars_df.id == cars_price_df.id,'left').select(cars_df['id'],cars_df['nama'],cars_price_df['car_price']).show()\n",
    "# right\n",
    "cars_df.join(cars_price_df,cars_df.id == cars_price_df.id,'right').select(cars_df['id'],cars_df['nama'],cars_price_df['car_price']).show()\n",
    "# full\n",
    "cars_df.join(cars_price_df,cars_df.id == cars_price_df.id,'full').select(cars_df['id'],cars_df['nama'],cars_price_df['car_price']).show()\n",
    "# outer\n",
    "cars_df.join(cars_price_df,cars_df.id == cars_price_df.id,'outer').select(cars_df['id'],cars_df['nama'],cars_price_df['car_price']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data CustomerOrders\n",
    "df = spark.read.csv('../context/CustomerOrders.csv',header=True,sep=\",\",inferSchema=True)\n",
    "\n",
    "# temp table\n",
    "df.createOrReplaceTempView(\"temp\")\n",
    "# # select all data from temp table\n",
    "# spark.sql(\"select * from temp limit 5\").show()\n",
    "# # select count of data in table\n",
    "# spark.sql(\"select count(*) as total_count from temp\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+--------------------+-------------+------+------------+---------+------------+-----------+---------------+---------+--------------+-------------+-----------+\n",
      "|CustomerKey| Toc|            FullName|MaritalStatus|Gender|YearlyIncome|Education|  Occupation|       City|  StateProvince|  Country|NumberOfOrders|TotalQuantity|TotalAmount|\n",
      "+-----------+----+--------------------+-------------+------+------------+---------+------------+-----------+---------------+---------+--------------+-------------+-----------+\n",
      "|      11000| Mr.|         Mr.Jon Yang|      Married|  Male| $90,000.00 |Bachelors|Professional|Rockhampton|     Queensland|Australia|             8|            8| $8,248.99 |\n",
      "|      11001| Mr.|     Mr.Eugene Huang|       Single|  Male| $60,000.00 |Bachelors|Professional|    Seaford|       Victoria|Australia|            11|           11| $6,383.88 |\n",
      "|      11002| Mr.|     Mr.Ruben Torres|      Married|  Male| $60,000.00 |Bachelors|Professional|     Hobart|       Tasmania|Australia|             4|            4| $8,114.04 |\n",
      "|      11003|Mrs.|     Mrs.Christy Zhu|       Single|Female| $70,000.00 |Bachelors|Professional| North Ryde|New South Wales|Australia|             9|            9| $8,139.29 |\n",
      "|      11004|Mrs.|Mrs.Elizabeth Joh...|       Single|Female| $80,000.00 |Bachelors|Professional| Wollongong|New South Wales|Australia|             6|            6| $8,196.01 |\n",
      "+-----------+----+--------------------+-------------+------+------------+---------+------------+-----------+---------------+---------+--------------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lakukan transformasi dari tabel temp\n",
    "df_transformed = spark.sql('''\n",
    "                select CustomerKey,\n",
    "                CASE\n",
    "                    WHEN Gender = 'F' AND MaritalStatus = 'M' THEN 'Miss.'\n",
    "                    WHEN Gender = 'F' AND MaritalStatus = 'S' THEN 'Mrs.'\n",
    "                    ELSE 'Mr.'\n",
    "                END AS Toc,\n",
    "                Concat(Toc,firstname,' ',lastname) as FullName,\n",
    "                case MaritalStatus\n",
    "                    when 'M' then 'Married'\n",
    "                    else 'Single'\n",
    "                end as MaritalStatus,\n",
    "                case Gender\n",
    "                    when 'M' then 'Male'\n",
    "                    else 'Female'\n",
    "                end as Gender,\n",
    "                YearlyIncome,\n",
    "                Education,\n",
    "                Occupation,\n",
    "                City,\n",
    "                StateProvince,\n",
    "                Country,\n",
    "                NumberOfOrders,\n",
    "                TotalQuantity,\n",
    "                TotalAmount\n",
    "                from temp limit 5''')\n",
    "df_transformed.show()\n",
    "\n",
    "# Simpan hasil transform ke temp_transform\n",
    "# spark.sql(\"select * from temp limit 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed = spark.sql('''select * , (Toc || fullname) as Full_Name\n",
    "                from temp2 limit 5''')\n",
    "df_transformed.show()\n",
    "df_transformed.createOrReplaceTempView(\"temp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+--------------------+-------------+------+------------+---------+------------+-----------+---------------+---------+--------------+-------------+-----------+-----------------+\n",
      "|CustomerKey| Toc|            FullName|MaritalStatus|Gender|yearlyincome|Education|  Occupation|       City|  StateProvince|  Country|NumberOfOrders|TotalQuantity|TotalAmount|   monthly_income|\n",
      "+-----------+----+--------------------+-------------+------+------------+---------+------------+-----------+---------------+---------+--------------+-------------+-----------+-----------------+\n",
      "|      11000| Mr.|         Mr.Jon Yang|      Married|  Male|       90000|Bachelors|Professional|Rockhampton|     Queensland|Australia|             8|            8| $8,248.99 |           7500.0|\n",
      "|      11001| Mr.|     Mr.Eugene Huang|       Single|  Male|       60000|Bachelors|Professional|    Seaford|       Victoria|Australia|            11|           11| $6,383.88 |           5000.0|\n",
      "|      11002| Mr.|     Mr.Ruben Torres|      Married|  Male|       60000|Bachelors|Professional|     Hobart|       Tasmania|Australia|             4|            4| $8,114.04 |           5000.0|\n",
      "|      11003|Mrs.|     Mrs.Christy Zhu|       Single|Female|       70000|Bachelors|Professional| North Ryde|New South Wales|Australia|             9|            9| $8,139.29 |5833.333333333333|\n",
      "|      11004|Mrs.|Mrs.Elizabeth Joh...|       Single|Female|       80000|Bachelors|Professional| Wollongong|New South Wales|Australia|             6|            6| $8,196.01 |6666.666666666667|\n",
      "+-----------+----+--------------------+-------------+------+------------+---------+------------+-----------+---------------+---------+--------------+-------------+-----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df_transformed = spark.sql('''\n",
    "#     SELECT *,\n",
    "#         REGEXP_REPLACE(yearlyincome, '\\\\.00', '0') AS yearlyincome\n",
    "#     FROM temp3\n",
    "# ''')\n",
    "# import fungsi sql yang diperukan seperti regex untuk menghapus simbol dan col untuk mengambil column\n",
    "from pyspark.sql.functions import regexp_replace, col\n",
    "df_transformed_2 = df_transformed.withColumn(\n",
    "    \"yearlyincome\",\n",
    "    regexp_replace(col(\"yearlyincome\"), r\"\\$\", \"\")  # Menghapus simbol dolar\n",
    ").withColumn(\n",
    "    \"yearlyincome\",\n",
    "    regexp_replace(col(\"yearlyincome\"), r\",\", \"\")  # Menghapus koma\n",
    ").withColumn(\n",
    "    \"yearlyincome\",\n",
    "    regexp_replace(col(\"yearlyincome\"), r\"\\.00\", \"\")  # Menghapus .00\n",
    ").withColumn(\n",
    "    \"yearlyincome\",\n",
    "    col(\"yearlyincome\").cast(\"integer\")  # Mengonversi kolom ke Integer\n",
    ").withColumn(\n",
    "    \"monthly_income\",\n",
    "    col(\"Yearlyincome\") / 12  # Menghitung monthly_income\n",
    ")\n",
    "\n",
    "df_transformed_2.show()\n",
    "\n",
    "# simpan hasil regex dan perhitungan montly income ke temp_output\n",
    "df_transformed_2.createOrReplaceTempView(\"temp_output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed = spark.sql('''\n",
    "                            select * ,\n",
    "                            (Yearlyincome/12) as montly_income \n",
    "                            from temp3\n",
    "                            ''')\n",
    "df_transformed.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
